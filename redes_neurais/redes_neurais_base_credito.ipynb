{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57Qk1hayDc8x"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Bases de dados/credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "8_Md39BMDxFq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCnN6_W-EAX6",
        "outputId": "804e7a99-d4fa-4ce0-ac11-9d4a0a8b3f6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJU171YoECCC",
        "outputId": "1f360d0b-7ab6-4259-df32-aa9455d03929"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 -> 100 -> 100 -> 1\n",
        "# 3 -> 2 -> 2 -> 1 (neuronio da classificaÃ§ao final)\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "                                   solver = 'adam', activation = 'relu',\n",
        "                                   hidden_layer_sizes = (20,20))\n",
        "#hidden_layer_sizes -> 2 camadas com 20 neuronios\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NpVVxHdLEHGg",
        "outputId": "ed390fce-dd27-482c-a680-0344f5dfcaa4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.65621783\n",
            "Iteration 2, loss = 0.60750553\n",
            "Iteration 3, loss = 0.56382237\n",
            "Iteration 4, loss = 0.52378594\n",
            "Iteration 5, loss = 0.48642400\n",
            "Iteration 6, loss = 0.45296783\n",
            "Iteration 7, loss = 0.42265018\n",
            "Iteration 8, loss = 0.39671440\n",
            "Iteration 9, loss = 0.37313938\n",
            "Iteration 10, loss = 0.35193429\n",
            "Iteration 11, loss = 0.33270311\n",
            "Iteration 12, loss = 0.31461424\n",
            "Iteration 13, loss = 0.29804448\n",
            "Iteration 14, loss = 0.28300335\n",
            "Iteration 15, loss = 0.26929475\n",
            "Iteration 16, loss = 0.25708050\n",
            "Iteration 17, loss = 0.24591061\n",
            "Iteration 18, loss = 0.23578785\n",
            "Iteration 19, loss = 0.22624044\n",
            "Iteration 20, loss = 0.21715040\n",
            "Iteration 21, loss = 0.20831372\n",
            "Iteration 22, loss = 0.20051489\n",
            "Iteration 23, loss = 0.19261798\n",
            "Iteration 24, loss = 0.18534757\n",
            "Iteration 25, loss = 0.17830460\n",
            "Iteration 26, loss = 0.17143489\n",
            "Iteration 27, loss = 0.16464022\n",
            "Iteration 28, loss = 0.15848064\n",
            "Iteration 29, loss = 0.15186191\n",
            "Iteration 30, loss = 0.14608633\n",
            "Iteration 31, loss = 0.14052055\n",
            "Iteration 32, loss = 0.13518639\n",
            "Iteration 33, loss = 0.13039973\n",
            "Iteration 34, loss = 0.12602219\n",
            "Iteration 35, loss = 0.12190160\n",
            "Iteration 36, loss = 0.11805635\n",
            "Iteration 37, loss = 0.11470026\n",
            "Iteration 38, loss = 0.11131151\n",
            "Iteration 39, loss = 0.10825167\n",
            "Iteration 40, loss = 0.10552238\n",
            "Iteration 41, loss = 0.10282043\n",
            "Iteration 42, loss = 0.10023933\n",
            "Iteration 43, loss = 0.09789410\n",
            "Iteration 44, loss = 0.09579068\n",
            "Iteration 45, loss = 0.09349618\n",
            "Iteration 46, loss = 0.09155220\n",
            "Iteration 47, loss = 0.08966855\n",
            "Iteration 48, loss = 0.08794179\n",
            "Iteration 49, loss = 0.08598267\n",
            "Iteration 50, loss = 0.08440565\n",
            "Iteration 51, loss = 0.08292615\n",
            "Iteration 52, loss = 0.08124880\n",
            "Iteration 53, loss = 0.07973104\n",
            "Iteration 54, loss = 0.07813168\n",
            "Iteration 55, loss = 0.07670878\n",
            "Iteration 56, loss = 0.07530249\n",
            "Iteration 57, loss = 0.07401263\n",
            "Iteration 58, loss = 0.07268868\n",
            "Iteration 59, loss = 0.07133811\n",
            "Iteration 60, loss = 0.07004198\n",
            "Iteration 61, loss = 0.06881490\n",
            "Iteration 62, loss = 0.06753117\n",
            "Iteration 63, loss = 0.06634551\n",
            "Iteration 64, loss = 0.06517535\n",
            "Iteration 65, loss = 0.06415185\n",
            "Iteration 66, loss = 0.06303049\n",
            "Iteration 67, loss = 0.06201515\n",
            "Iteration 68, loss = 0.06077051\n",
            "Iteration 69, loss = 0.05983340\n",
            "Iteration 70, loss = 0.05883613\n",
            "Iteration 71, loss = 0.05784090\n",
            "Iteration 72, loss = 0.05698927\n",
            "Iteration 73, loss = 0.05607357\n",
            "Iteration 74, loss = 0.05510086\n",
            "Iteration 75, loss = 0.05425627\n",
            "Iteration 76, loss = 0.05342933\n",
            "Iteration 77, loss = 0.05259115\n",
            "Iteration 78, loss = 0.05169431\n",
            "Iteration 79, loss = 0.05091484\n",
            "Iteration 80, loss = 0.05023116\n",
            "Iteration 81, loss = 0.04940431\n",
            "Iteration 82, loss = 0.04873031\n",
            "Iteration 83, loss = 0.04799897\n",
            "Iteration 84, loss = 0.04734154\n",
            "Iteration 85, loss = 0.04669249\n",
            "Iteration 86, loss = 0.04605990\n",
            "Iteration 87, loss = 0.04547707\n",
            "Iteration 88, loss = 0.04471561\n",
            "Iteration 89, loss = 0.04408391\n",
            "Iteration 90, loss = 0.04344023\n",
            "Iteration 91, loss = 0.04321215\n",
            "Iteration 92, loss = 0.04225545\n",
            "Iteration 93, loss = 0.04176778\n",
            "Iteration 94, loss = 0.04114181\n",
            "Iteration 95, loss = 0.04058612\n",
            "Iteration 96, loss = 0.04005487\n",
            "Iteration 97, loss = 0.03953318\n",
            "Iteration 98, loss = 0.03900095\n",
            "Iteration 99, loss = 0.03857257\n",
            "Iteration 100, loss = 0.03809505\n",
            "Iteration 101, loss = 0.03770039\n",
            "Iteration 102, loss = 0.03709415\n",
            "Iteration 103, loss = 0.03660103\n",
            "Iteration 104, loss = 0.03620481\n",
            "Iteration 105, loss = 0.03590896\n",
            "Iteration 106, loss = 0.03531585\n",
            "Iteration 107, loss = 0.03499028\n",
            "Iteration 108, loss = 0.03444458\n",
            "Iteration 109, loss = 0.03399368\n",
            "Iteration 110, loss = 0.03371180\n",
            "Iteration 111, loss = 0.03325737\n",
            "Iteration 112, loss = 0.03294551\n",
            "Iteration 113, loss = 0.03252223\n",
            "Iteration 114, loss = 0.03232807\n",
            "Iteration 115, loss = 0.03164074\n",
            "Iteration 116, loss = 0.03137120\n",
            "Iteration 117, loss = 0.03102162\n",
            "Iteration 118, loss = 0.03063017\n",
            "Iteration 119, loss = 0.03031656\n",
            "Iteration 120, loss = 0.02994914\n",
            "Iteration 121, loss = 0.02969190\n",
            "Iteration 122, loss = 0.02932571\n",
            "Iteration 123, loss = 0.02902078\n",
            "Iteration 124, loss = 0.02863596\n",
            "Iteration 125, loss = 0.02838900\n",
            "Iteration 126, loss = 0.02810454\n",
            "Iteration 127, loss = 0.02790009\n",
            "Iteration 128, loss = 0.02751574\n",
            "Iteration 129, loss = 0.02728145\n",
            "Iteration 130, loss = 0.02718665\n",
            "Iteration 131, loss = 0.02712693\n",
            "Iteration 132, loss = 0.02637756\n",
            "Iteration 133, loss = 0.02605201\n",
            "Iteration 134, loss = 0.02582456\n",
            "Iteration 135, loss = 0.02563450\n",
            "Iteration 136, loss = 0.02548046\n",
            "Iteration 137, loss = 0.02511531\n",
            "Iteration 138, loss = 0.02507430\n",
            "Iteration 139, loss = 0.02481444\n",
            "Iteration 140, loss = 0.02448396\n",
            "Iteration 141, loss = 0.02425359\n",
            "Iteration 142, loss = 0.02395024\n",
            "Iteration 143, loss = 0.02383021\n",
            "Iteration 144, loss = 0.02354368\n",
            "Iteration 145, loss = 0.02345573\n",
            "Iteration 146, loss = 0.02318060\n",
            "Iteration 147, loss = 0.02295414\n",
            "Iteration 148, loss = 0.02279077\n",
            "Iteration 149, loss = 0.02258064\n",
            "Iteration 150, loss = 0.02250290\n",
            "Iteration 151, loss = 0.02228774\n",
            "Iteration 152, loss = 0.02204432\n",
            "Iteration 153, loss = 0.02198536\n",
            "Iteration 154, loss = 0.02169428\n",
            "Iteration 155, loss = 0.02143963\n",
            "Iteration 156, loss = 0.02133936\n",
            "Iteration 157, loss = 0.02112223\n",
            "Iteration 158, loss = 0.02112956\n",
            "Iteration 159, loss = 0.02077808\n",
            "Iteration 160, loss = 0.02063631\n",
            "Iteration 161, loss = 0.02051181\n",
            "Iteration 162, loss = 0.02030611\n",
            "Iteration 163, loss = 0.02010816\n",
            "Iteration 164, loss = 0.01992763\n",
            "Iteration 165, loss = 0.01986860\n",
            "Iteration 166, loss = 0.01976203\n",
            "Iteration 167, loss = 0.01959861\n",
            "Iteration 168, loss = 0.01942735\n",
            "Iteration 169, loss = 0.01935429\n",
            "Iteration 170, loss = 0.01904396\n",
            "Iteration 171, loss = 0.01910487\n",
            "Iteration 172, loss = 0.01878691\n",
            "Iteration 173, loss = 0.01873393\n",
            "Iteration 174, loss = 0.01851213\n",
            "Iteration 175, loss = 0.01840154\n",
            "Iteration 176, loss = 0.01824995\n",
            "Iteration 177, loss = 0.01815057\n",
            "Iteration 178, loss = 0.01796189\n",
            "Iteration 179, loss = 0.01796319\n",
            "Iteration 180, loss = 0.01771284\n",
            "Iteration 181, loss = 0.01775820\n",
            "Iteration 182, loss = 0.01754643\n",
            "Iteration 183, loss = 0.01761782\n",
            "Iteration 184, loss = 0.01730265\n",
            "Iteration 185, loss = 0.01738119\n",
            "Iteration 186, loss = 0.01710408\n",
            "Iteration 187, loss = 0.01708401\n",
            "Iteration 188, loss = 0.01705119\n",
            "Iteration 189, loss = 0.01684627\n",
            "Iteration 190, loss = 0.01663930\n",
            "Iteration 191, loss = 0.01646331\n",
            "Iteration 192, loss = 0.01636692\n",
            "Iteration 193, loss = 0.01628384\n",
            "Iteration 194, loss = 0.01626868\n",
            "Iteration 195, loss = 0.01603977\n",
            "Iteration 196, loss = 0.01609590\n",
            "Iteration 197, loss = 0.01588387\n",
            "Iteration 198, loss = 0.01584181\n",
            "Iteration 199, loss = 0.01563008\n",
            "Iteration 200, loss = 0.01557214\n",
            "Iteration 201, loss = 0.01544993\n",
            "Iteration 202, loss = 0.01553633\n",
            "Iteration 203, loss = 0.01528860\n",
            "Iteration 204, loss = 0.01535086\n",
            "Iteration 205, loss = 0.01508340\n",
            "Iteration 206, loss = 0.01508086\n",
            "Iteration 207, loss = 0.01491373\n",
            "Iteration 208, loss = 0.01489734\n",
            "Iteration 209, loss = 0.01477613\n",
            "Iteration 210, loss = 0.01479005\n",
            "Iteration 211, loss = 0.01477452\n",
            "Iteration 212, loss = 0.01445101\n",
            "Iteration 213, loss = 0.01438396\n",
            "Iteration 214, loss = 0.01432291\n",
            "Iteration 215, loss = 0.01456996\n",
            "Iteration 216, loss = 0.01402760\n",
            "Iteration 217, loss = 0.01406312\n",
            "Iteration 218, loss = 0.01402369\n",
            "Iteration 219, loss = 0.01391291\n",
            "Iteration 220, loss = 0.01399862\n",
            "Iteration 221, loss = 0.01387956\n",
            "Iteration 222, loss = 0.01384204\n",
            "Iteration 223, loss = 0.01357123\n",
            "Iteration 224, loss = 0.01355868\n",
            "Iteration 225, loss = 0.01357327\n",
            "Iteration 226, loss = 0.01345193\n",
            "Iteration 227, loss = 0.01335573\n",
            "Iteration 228, loss = 0.01313783\n",
            "Iteration 229, loss = 0.01342941\n",
            "Iteration 230, loss = 0.01302067\n",
            "Iteration 231, loss = 0.01313408\n",
            "Iteration 232, loss = 0.01298366\n",
            "Iteration 233, loss = 0.01284633\n",
            "Iteration 234, loss = 0.01287555\n",
            "Iteration 235, loss = 0.01266315\n",
            "Iteration 236, loss = 0.01277970\n",
            "Iteration 237, loss = 0.01281750\n",
            "Iteration 238, loss = 0.01268267\n",
            "Iteration 239, loss = 0.01260455\n",
            "Iteration 240, loss = 0.01241800\n",
            "Iteration 241, loss = 0.01242570\n",
            "Iteration 242, loss = 0.01230912\n",
            "Iteration 243, loss = 0.01219679\n",
            "Iteration 244, loss = 0.01216128\n",
            "Iteration 245, loss = 0.01228322\n",
            "Iteration 246, loss = 0.01204385\n",
            "Iteration 247, loss = 0.01187101\n",
            "Iteration 248, loss = 0.01187362\n",
            "Iteration 249, loss = 0.01184104\n",
            "Iteration 250, loss = 0.01174347\n",
            "Iteration 251, loss = 0.01168287\n",
            "Iteration 252, loss = 0.01185454\n",
            "Iteration 253, loss = 0.01149584\n",
            "Iteration 254, loss = 0.01176155\n",
            "Iteration 255, loss = 0.01136962\n",
            "Iteration 256, loss = 0.01172512\n",
            "Iteration 257, loss = 0.01149748\n",
            "Iteration 258, loss = 0.01146052\n",
            "Iteration 259, loss = 0.01129182\n",
            "Iteration 260, loss = 0.01117801\n",
            "Iteration 261, loss = 0.01130622\n",
            "Iteration 262, loss = 0.01100623\n",
            "Iteration 263, loss = 0.01142297\n",
            "Iteration 264, loss = 0.01086999\n",
            "Iteration 265, loss = 0.01101050\n",
            "Iteration 266, loss = 0.01084579\n",
            "Iteration 267, loss = 0.01077440\n",
            "Iteration 268, loss = 0.01080645\n",
            "Iteration 269, loss = 0.01070297\n",
            "Iteration 270, loss = 0.01063597\n",
            "Iteration 271, loss = 0.01073410\n",
            "Iteration 272, loss = 0.01051611\n",
            "Iteration 273, loss = 0.01059620\n",
            "Iteration 274, loss = 0.01047705\n",
            "Iteration 275, loss = 0.01042321\n",
            "Iteration 276, loss = 0.01027294\n",
            "Iteration 277, loss = 0.01037722\n",
            "Iteration 278, loss = 0.01038833\n",
            "Iteration 279, loss = 0.01022147\n",
            "Iteration 280, loss = 0.01019585\n",
            "Iteration 281, loss = 0.01008351\n",
            "Iteration 282, loss = 0.01000538\n",
            "Iteration 283, loss = 0.01000391\n",
            "Iteration 284, loss = 0.00997272\n",
            "Iteration 285, loss = 0.00991223\n",
            "Iteration 286, loss = 0.00994261\n",
            "Iteration 287, loss = 0.01008717\n",
            "Iteration 288, loss = 0.00984384\n",
            "Iteration 289, loss = 0.00981419\n",
            "Iteration 290, loss = 0.00975043\n",
            "Iteration 291, loss = 0.00968330\n",
            "Iteration 292, loss = 0.01011909\n",
            "Iteration 293, loss = 0.00959358\n",
            "Iteration 294, loss = 0.00991110\n",
            "Iteration 295, loss = 0.00956687\n",
            "Iteration 296, loss = 0.00948659\n",
            "Iteration 297, loss = 0.00936120\n",
            "Iteration 298, loss = 0.00948921\n",
            "Iteration 299, loss = 0.00934630\n",
            "Iteration 300, loss = 0.00922990\n",
            "Iteration 301, loss = 0.00918535\n",
            "Iteration 302, loss = 0.00917559\n",
            "Iteration 303, loss = 0.00925946\n",
            "Iteration 304, loss = 0.00921129\n",
            "Iteration 305, loss = 0.00904693\n",
            "Iteration 306, loss = 0.00896703\n",
            "Iteration 307, loss = 0.00895614\n",
            "Iteration 308, loss = 0.00887318\n",
            "Iteration 309, loss = 0.00898497\n",
            "Iteration 310, loss = 0.00884678\n",
            "Iteration 311, loss = 0.00886060\n",
            "Iteration 312, loss = 0.00875335\n",
            "Iteration 313, loss = 0.00878708\n",
            "Iteration 314, loss = 0.00868830\n",
            "Iteration 315, loss = 0.00864068\n",
            "Iteration 316, loss = 0.00865605\n",
            "Iteration 317, loss = 0.00863137\n",
            "Iteration 318, loss = 0.00865342\n",
            "Iteration 319, loss = 0.00862672\n",
            "Iteration 320, loss = 0.00872961\n",
            "Iteration 321, loss = 0.00851881\n",
            "Iteration 322, loss = 0.00852267\n",
            "Iteration 323, loss = 0.00834554\n",
            "Iteration 324, loss = 0.00838210\n",
            "Iteration 325, loss = 0.00827283\n",
            "Iteration 326, loss = 0.00827285\n",
            "Iteration 327, loss = 0.00838537\n",
            "Iteration 328, loss = 0.00830190\n",
            "Iteration 329, loss = 0.00811183\n",
            "Iteration 330, loss = 0.00815397\n",
            "Iteration 331, loss = 0.00807584\n",
            "Iteration 332, loss = 0.00815622\n",
            "Iteration 333, loss = 0.00813608\n",
            "Iteration 334, loss = 0.00798720\n",
            "Iteration 335, loss = 0.00801291\n",
            "Iteration 336, loss = 0.00802774\n",
            "Iteration 337, loss = 0.00788360\n",
            "Iteration 338, loss = 0.00793053\n",
            "Iteration 339, loss = 0.00788685\n",
            "Iteration 340, loss = 0.00809130\n",
            "Iteration 341, loss = 0.00772294\n",
            "Iteration 342, loss = 0.00775818\n",
            "Iteration 343, loss = 0.00774593\n",
            "Iteration 344, loss = 0.00770717\n",
            "Iteration 345, loss = 0.00763427\n",
            "Iteration 346, loss = 0.00762276\n",
            "Iteration 347, loss = 0.00758250\n",
            "Iteration 348, loss = 0.00757694\n",
            "Iteration 349, loss = 0.00750236\n",
            "Iteration 350, loss = 0.00748024\n",
            "Iteration 351, loss = 0.00743673\n",
            "Iteration 352, loss = 0.00752114\n",
            "Iteration 353, loss = 0.00735886\n",
            "Iteration 354, loss = 0.00745173\n",
            "Iteration 355, loss = 0.00742405\n",
            "Iteration 356, loss = 0.00737911\n",
            "Iteration 357, loss = 0.00732609\n",
            "Iteration 358, loss = 0.00718308\n",
            "Iteration 359, loss = 0.00723193\n",
            "Iteration 360, loss = 0.00730660\n",
            "Iteration 361, loss = 0.00718627\n",
            "Iteration 362, loss = 0.00712329\n",
            "Iteration 363, loss = 0.00716662\n",
            "Iteration 364, loss = 0.00702665\n",
            "Iteration 365, loss = 0.00712942\n",
            "Iteration 366, loss = 0.00705047\n",
            "Iteration 367, loss = 0.00699847\n",
            "Iteration 368, loss = 0.00694898\n",
            "Iteration 369, loss = 0.00691888\n",
            "Iteration 370, loss = 0.00688440\n",
            "Iteration 371, loss = 0.00707040\n",
            "Iteration 372, loss = 0.00697475\n",
            "Iteration 373, loss = 0.00712059\n",
            "Iteration 374, loss = 0.00674378\n",
            "Iteration 375, loss = 0.00683762\n",
            "Iteration 376, loss = 0.00687035\n",
            "Iteration 377, loss = 0.00668254\n",
            "Iteration 378, loss = 0.00668000\n",
            "Iteration 379, loss = 0.00668606\n",
            "Iteration 380, loss = 0.00664278\n",
            "Iteration 381, loss = 0.00675080\n",
            "Iteration 382, loss = 0.00665633\n",
            "Iteration 383, loss = 0.00662981\n",
            "Iteration 384, loss = 0.00649805\n",
            "Iteration 385, loss = 0.00657260\n",
            "Iteration 386, loss = 0.00643197\n",
            "Iteration 387, loss = 0.00643286\n",
            "Iteration 388, loss = 0.00640601\n",
            "Iteration 389, loss = 0.00645524\n",
            "Iteration 390, loss = 0.00642435\n",
            "Iteration 391, loss = 0.00643130\n",
            "Iteration 392, loss = 0.00628772\n",
            "Iteration 393, loss = 0.00629265\n",
            "Iteration 394, loss = 0.00635372\n",
            "Iteration 395, loss = 0.00630153\n",
            "Iteration 396, loss = 0.00628748\n",
            "Iteration 397, loss = 0.00635603\n",
            "Iteration 398, loss = 0.00624122\n",
            "Iteration 399, loss = 0.00616662\n",
            "Iteration 400, loss = 0.00641374\n",
            "Iteration 401, loss = 0.00609705\n",
            "Iteration 402, loss = 0.00616696\n",
            "Iteration 403, loss = 0.00615251\n",
            "Iteration 404, loss = 0.00599919\n",
            "Iteration 405, loss = 0.00599858\n",
            "Iteration 406, loss = 0.00606224\n",
            "Iteration 407, loss = 0.00601288\n",
            "Iteration 408, loss = 0.00619835\n",
            "Iteration 409, loss = 0.00596245\n",
            "Iteration 410, loss = 0.00600595\n",
            "Iteration 411, loss = 0.00573528\n",
            "Iteration 412, loss = 0.00598644\n",
            "Iteration 413, loss = 0.00587052\n",
            "Iteration 414, loss = 0.00592490\n",
            "Iteration 415, loss = 0.00570285\n",
            "Iteration 416, loss = 0.00583667\n",
            "Iteration 417, loss = 0.00597037\n",
            "Iteration 418, loss = 0.00577472\n",
            "Iteration 419, loss = 0.00572246\n",
            "Iteration 420, loss = 0.00572742\n",
            "Iteration 421, loss = 0.00567982\n",
            "Iteration 422, loss = 0.00580037\n",
            "Iteration 423, loss = 0.00565256\n",
            "Iteration 424, loss = 0.00557792\n",
            "Iteration 425, loss = 0.00565322\n",
            "Iteration 426, loss = 0.00556289\n",
            "Iteration 427, loss = 0.00576813\n",
            "Iteration 428, loss = 0.00552793\n",
            "Iteration 429, loss = 0.00549123\n",
            "Iteration 430, loss = 0.00551847\n",
            "Iteration 431, loss = 0.00545983\n",
            "Iteration 432, loss = 0.00542041\n",
            "Iteration 433, loss = 0.00540276\n",
            "Iteration 434, loss = 0.00535126\n",
            "Iteration 435, loss = 0.00544532\n",
            "Iteration 436, loss = 0.00536626\n",
            "Iteration 437, loss = 0.00534021\n",
            "Iteration 438, loss = 0.00540269\n",
            "Iteration 439, loss = 0.00536094\n",
            "Iteration 440, loss = 0.00530949\n",
            "Iteration 441, loss = 0.00523352\n",
            "Iteration 442, loss = 0.00534328\n",
            "Iteration 443, loss = 0.00528935\n",
            "Iteration 444, loss = 0.00516237\n",
            "Iteration 445, loss = 0.00513632\n",
            "Iteration 446, loss = 0.00516099\n",
            "Iteration 447, loss = 0.00519463\n",
            "Iteration 448, loss = 0.00512530\n",
            "Iteration 449, loss = 0.00516352\n",
            "Iteration 450, loss = 0.00503302\n",
            "Iteration 451, loss = 0.00517370\n",
            "Iteration 452, loss = 0.00528538\n",
            "Iteration 453, loss = 0.00504125\n",
            "Iteration 454, loss = 0.00525387\n",
            "Iteration 455, loss = 0.00508022\n",
            "Iteration 456, loss = 0.00496802\n",
            "Iteration 457, loss = 0.00497182\n",
            "Iteration 458, loss = 0.00490778\n",
            "Iteration 459, loss = 0.00497552\n",
            "Iteration 460, loss = 0.00491618\n",
            "Iteration 461, loss = 0.00490601\n",
            "Iteration 462, loss = 0.00484624\n",
            "Iteration 463, loss = 0.00487136\n",
            "Iteration 464, loss = 0.00479034\n",
            "Iteration 465, loss = 0.00476911\n",
            "Iteration 466, loss = 0.00487534\n",
            "Iteration 467, loss = 0.00496761\n",
            "Iteration 468, loss = 0.00468554\n",
            "Iteration 469, loss = 0.00475817\n",
            "Iteration 470, loss = 0.00482790\n",
            "Iteration 471, loss = 0.00464645\n",
            "Iteration 472, loss = 0.00466074\n",
            "Iteration 473, loss = 0.00466610\n",
            "Iteration 474, loss = 0.00462434\n",
            "Iteration 475, loss = 0.00465803\n",
            "Iteration 476, loss = 0.00473432\n",
            "Iteration 477, loss = 0.00458575\n",
            "Iteration 478, loss = 0.00456734\n",
            "Iteration 479, loss = 0.00457093\n",
            "Iteration 480, loss = 0.00460284\n",
            "Iteration 481, loss = 0.00456776\n",
            "Iteration 482, loss = 0.00445759\n",
            "Iteration 483, loss = 0.00448307\n",
            "Iteration 484, loss = 0.00451267\n",
            "Iteration 485, loss = 0.00439275\n",
            "Iteration 486, loss = 0.00441710\n",
            "Iteration 487, loss = 0.00440816\n",
            "Iteration 488, loss = 0.00441160\n",
            "Iteration 489, loss = 0.00437836\n",
            "Iteration 490, loss = 0.00439308\n",
            "Iteration 491, loss = 0.00438397\n",
            "Iteration 492, loss = 0.00434267\n",
            "Iteration 493, loss = 0.00443471\n",
            "Iteration 494, loss = 0.00431219\n",
            "Iteration 495, loss = 0.00450278\n",
            "Iteration 496, loss = 0.00435784\n",
            "Iteration 497, loss = 0.00442130\n",
            "Iteration 498, loss = 0.00425792\n",
            "Iteration 499, loss = 0.00438622\n",
            "Iteration 500, loss = 0.00427743\n",
            "Iteration 501, loss = 0.00420295\n",
            "Iteration 502, loss = 0.00437275\n",
            "Iteration 503, loss = 0.00420137\n",
            "Iteration 504, loss = 0.00430075\n",
            "Iteration 505, loss = 0.00411565\n",
            "Iteration 506, loss = 0.00418001\n",
            "Iteration 507, loss = 0.00414541\n",
            "Iteration 508, loss = 0.00414335\n",
            "Iteration 509, loss = 0.00404692\n",
            "Iteration 510, loss = 0.00411355\n",
            "Iteration 511, loss = 0.00424217\n",
            "Iteration 512, loss = 0.00420746\n",
            "Iteration 513, loss = 0.00406630\n",
            "Iteration 514, loss = 0.00400751\n",
            "Iteration 515, loss = 0.00406774\n",
            "Iteration 516, loss = 0.00400000\n",
            "Iteration 517, loss = 0.00396442\n",
            "Iteration 518, loss = 0.00394850\n",
            "Iteration 519, loss = 0.00398981\n",
            "Iteration 520, loss = 0.00392570\n",
            "Iteration 521, loss = 0.00394651\n",
            "Iteration 522, loss = 0.00398696\n",
            "Iteration 523, loss = 0.00383709\n",
            "Iteration 524, loss = 0.00395209\n",
            "Iteration 525, loss = 0.00389286\n",
            "Iteration 526, loss = 0.00391900\n",
            "Iteration 527, loss = 0.00396367\n",
            "Iteration 528, loss = 0.00381565\n",
            "Iteration 529, loss = 0.00384555\n",
            "Iteration 530, loss = 0.00380754\n",
            "Iteration 531, loss = 0.00381248\n",
            "Iteration 532, loss = 0.00388017\n",
            "Iteration 533, loss = 0.00368639\n",
            "Iteration 534, loss = 0.00369674\n",
            "Iteration 535, loss = 0.00375366\n",
            "Iteration 536, loss = 0.00366781\n",
            "Iteration 537, loss = 0.00371342\n",
            "Iteration 538, loss = 0.00378504\n",
            "Iteration 539, loss = 0.00373949\n",
            "Iteration 540, loss = 0.00363343\n",
            "Iteration 541, loss = 0.00369399\n",
            "Iteration 542, loss = 0.00374908\n",
            "Iteration 543, loss = 0.00363685\n",
            "Iteration 544, loss = 0.00363747\n",
            "Iteration 545, loss = 0.00374149\n",
            "Iteration 546, loss = 0.00382139\n",
            "Iteration 547, loss = 0.00366879\n",
            "Iteration 548, loss = 0.00360824\n",
            "Iteration 549, loss = 0.00358195\n",
            "Iteration 550, loss = 0.00353353\n",
            "Iteration 551, loss = 0.00352279\n",
            "Iteration 552, loss = 0.00357909\n",
            "Iteration 553, loss = 0.00379773\n",
            "Iteration 554, loss = 0.00343837\n",
            "Iteration 555, loss = 0.00359348\n",
            "Iteration 556, loss = 0.00372140\n",
            "Iteration 557, loss = 0.00356756\n",
            "Iteration 558, loss = 0.00342683\n",
            "Iteration 559, loss = 0.00346253\n",
            "Iteration 560, loss = 0.00353272\n",
            "Iteration 561, loss = 0.00355180\n",
            "Iteration 562, loss = 0.00339197\n",
            "Iteration 563, loss = 0.00349573\n",
            "Iteration 564, loss = 0.00333443\n",
            "Iteration 565, loss = 0.00339060\n",
            "Iteration 566, loss = 0.00332362\n",
            "Iteration 567, loss = 0.00334840\n",
            "Iteration 568, loss = 0.00325347\n",
            "Iteration 569, loss = 0.00347195\n",
            "Iteration 570, loss = 0.00328318\n",
            "Iteration 571, loss = 0.00330990\n",
            "Iteration 572, loss = 0.00326222\n",
            "Iteration 573, loss = 0.00331653\n",
            "Iteration 574, loss = 0.00327262\n",
            "Iteration 575, loss = 0.00327923\n",
            "Iteration 576, loss = 0.00323749\n",
            "Iteration 577, loss = 0.00333962\n",
            "Iteration 578, loss = 0.00315494\n",
            "Iteration 579, loss = 0.00317252\n",
            "Iteration 580, loss = 0.00324548\n",
            "Iteration 581, loss = 0.00317883\n",
            "Iteration 582, loss = 0.00319066\n",
            "Iteration 583, loss = 0.00317639\n",
            "Iteration 584, loss = 0.00304286\n",
            "Iteration 585, loss = 0.00323325\n",
            "Iteration 586, loss = 0.00307954\n",
            "Iteration 587, loss = 0.00316832\n",
            "Iteration 588, loss = 0.00314929\n",
            "Iteration 589, loss = 0.00308765\n",
            "Iteration 590, loss = 0.00309182\n",
            "Iteration 591, loss = 0.00304047\n",
            "Iteration 592, loss = 0.00316050\n",
            "Iteration 593, loss = 0.00302896\n",
            "Iteration 594, loss = 0.00302639\n",
            "Iteration 595, loss = 0.00307784\n",
            "Iteration 596, loss = 0.00296600\n",
            "Iteration 597, loss = 0.00316241\n",
            "Iteration 598, loss = 0.00297336\n",
            "Iteration 599, loss = 0.00298856\n",
            "Iteration 600, loss = 0.00309711\n",
            "Iteration 601, loss = 0.00291250\n",
            "Iteration 602, loss = 0.00297563\n",
            "Iteration 603, loss = 0.00295266\n",
            "Iteration 604, loss = 0.00300702\n",
            "Iteration 605, loss = 0.00304995\n",
            "Iteration 606, loss = 0.00299623\n",
            "Iteration 607, loss = 0.00299545\n",
            "Iteration 608, loss = 0.00290342\n",
            "Iteration 609, loss = 0.00288045\n",
            "Iteration 610, loss = 0.00293639\n",
            "Iteration 611, loss = 0.00285810\n",
            "Iteration 612, loss = 0.00281643\n",
            "Iteration 613, loss = 0.00293605\n",
            "Iteration 614, loss = 0.00296201\n",
            "Iteration 615, loss = 0.00286018\n",
            "Iteration 616, loss = 0.00282660\n",
            "Iteration 617, loss = 0.00276725\n",
            "Iteration 618, loss = 0.00288266\n",
            "Iteration 619, loss = 0.00285183\n",
            "Iteration 620, loss = 0.00275097\n",
            "Iteration 621, loss = 0.00274826\n",
            "Iteration 622, loss = 0.00276023\n",
            "Iteration 623, loss = 0.00276585\n",
            "Iteration 624, loss = 0.00269161\n",
            "Iteration 625, loss = 0.00273959\n",
            "Iteration 626, loss = 0.00270738\n",
            "Iteration 627, loss = 0.00270714\n",
            "Iteration 628, loss = 0.00268228\n",
            "Iteration 629, loss = 0.00273915\n",
            "Iteration 630, loss = 0.00272484\n",
            "Iteration 631, loss = 0.00265164\n",
            "Iteration 632, loss = 0.00261707\n",
            "Iteration 633, loss = 0.00266556\n",
            "Iteration 634, loss = 0.00266837\n",
            "Iteration 635, loss = 0.00263774\n",
            "Iteration 636, loss = 0.00268527\n",
            "Iteration 637, loss = 0.00262427\n",
            "Iteration 638, loss = 0.00265703\n",
            "Iteration 639, loss = 0.00259431\n",
            "Iteration 640, loss = 0.00262366\n",
            "Iteration 641, loss = 0.00255819\n",
            "Iteration 642, loss = 0.00257024\n",
            "Iteration 643, loss = 0.00254644\n",
            "Iteration 644, loss = 0.00253792\n",
            "Iteration 645, loss = 0.00254184\n",
            "Iteration 646, loss = 0.00257347\n",
            "Iteration 647, loss = 0.00251225\n",
            "Iteration 648, loss = 0.00262412\n",
            "Iteration 649, loss = 0.00249733\n",
            "Iteration 650, loss = 0.00253242\n",
            "Iteration 651, loss = 0.00250443\n",
            "Iteration 652, loss = 0.00254767\n",
            "Iteration 653, loss = 0.00246000\n",
            "Iteration 654, loss = 0.00246826\n",
            "Iteration 655, loss = 0.00248780\n",
            "Iteration 656, loss = 0.00241711\n",
            "Iteration 657, loss = 0.00247235\n",
            "Iteration 658, loss = 0.00249734\n",
            "Iteration 659, loss = 0.00245364\n",
            "Iteration 660, loss = 0.00239783\n",
            "Iteration 661, loss = 0.00251240\n",
            "Iteration 662, loss = 0.00251625\n",
            "Iteration 663, loss = 0.00241535\n",
            "Iteration 664, loss = 0.00241061\n",
            "Iteration 665, loss = 0.00238473\n",
            "Iteration 666, loss = 0.00237023\n",
            "Iteration 667, loss = 0.00235661\n",
            "Iteration 668, loss = 0.00242072\n",
            "Iteration 669, loss = 0.00241280\n",
            "Iteration 670, loss = 0.00244712\n",
            "Iteration 671, loss = 0.00236128\n",
            "Iteration 672, loss = 0.00233112\n",
            "Iteration 673, loss = 0.00233345\n",
            "Iteration 674, loss = 0.00230678\n",
            "Iteration 675, loss = 0.00231525\n",
            "Iteration 676, loss = 0.00250884\n",
            "Iteration 677, loss = 0.00227964\n",
            "Iteration 678, loss = 0.00241112\n",
            "Iteration 679, loss = 0.00230820\n",
            "Iteration 680, loss = 0.00228406\n",
            "Iteration 681, loss = 0.00223826\n",
            "Iteration 682, loss = 0.00226298\n",
            "Iteration 683, loss = 0.00232309\n",
            "Iteration 684, loss = 0.00234196\n",
            "Iteration 685, loss = 0.00222883\n",
            "Iteration 686, loss = 0.00224422\n",
            "Iteration 687, loss = 0.00223569\n",
            "Iteration 688, loss = 0.00230897\n",
            "Iteration 689, loss = 0.00219067\n",
            "Iteration 690, loss = 0.00230033\n",
            "Iteration 691, loss = 0.00220275\n",
            "Iteration 692, loss = 0.00221163\n",
            "Iteration 693, loss = 0.00228602\n",
            "Iteration 694, loss = 0.00216045\n",
            "Iteration 695, loss = 0.00219362\n",
            "Iteration 696, loss = 0.00217700\n",
            "Iteration 697, loss = 0.00218983\n",
            "Iteration 698, loss = 0.00218525\n",
            "Iteration 699, loss = 0.00209777\n",
            "Iteration 700, loss = 0.00215414\n",
            "Iteration 701, loss = 0.00216332\n",
            "Iteration 702, loss = 0.00220845\n",
            "Iteration 703, loss = 0.00217444\n",
            "Iteration 704, loss = 0.00211526\n",
            "Iteration 705, loss = 0.00215637\n",
            "Iteration 706, loss = 0.00211519\n",
            "Iteration 707, loss = 0.00212256\n",
            "Iteration 708, loss = 0.00211377\n",
            "Iteration 709, loss = 0.00211800\n",
            "Iteration 710, loss = 0.00205069\n",
            "Iteration 711, loss = 0.00206491\n",
            "Iteration 712, loss = 0.00202689\n",
            "Iteration 713, loss = 0.00207349\n",
            "Iteration 714, loss = 0.00210423\n",
            "Iteration 715, loss = 0.00203682\n",
            "Iteration 716, loss = 0.00202918\n",
            "Iteration 717, loss = 0.00199293\n",
            "Iteration 718, loss = 0.00206426\n",
            "Iteration 719, loss = 0.00198921\n",
            "Iteration 720, loss = 0.00199695\n",
            "Iteration 721, loss = 0.00196623\n",
            "Iteration 722, loss = 0.00204547\n",
            "Iteration 723, loss = 0.00197096\n",
            "Iteration 724, loss = 0.00200566\n",
            "Iteration 725, loss = 0.00195005\n",
            "Iteration 726, loss = 0.00200149\n",
            "Iteration 727, loss = 0.00200729\n",
            "Iteration 728, loss = 0.00193170\n",
            "Iteration 729, loss = 0.00204358\n",
            "Iteration 730, loss = 0.00188624\n",
            "Iteration 731, loss = 0.00192888\n",
            "Iteration 732, loss = 0.00192699\n",
            "Iteration 733, loss = 0.00190344\n",
            "Iteration 734, loss = 0.00194781\n",
            "Iteration 735, loss = 0.00194098\n",
            "Iteration 736, loss = 0.00202988\n",
            "Iteration 737, loss = 0.00185681\n",
            "Iteration 738, loss = 0.00197047\n",
            "Iteration 739, loss = 0.00189767\n",
            "Iteration 740, loss = 0.00193174\n",
            "Iteration 741, loss = 0.00186979\n",
            "Iteration 742, loss = 0.00187934\n",
            "Iteration 743, loss = 0.00187882\n",
            "Iteration 744, loss = 0.00196888\n",
            "Iteration 745, loss = 0.00187874\n",
            "Iteration 746, loss = 0.00197941\n",
            "Iteration 747, loss = 0.00181891\n",
            "Iteration 748, loss = 0.00182650\n",
            "Iteration 749, loss = 0.00188345\n",
            "Iteration 750, loss = 0.00192464\n",
            "Iteration 751, loss = 0.00183651\n",
            "Iteration 752, loss = 0.00184592\n",
            "Iteration 753, loss = 0.00179664\n",
            "Iteration 754, loss = 0.00178175\n",
            "Iteration 755, loss = 0.00182374\n",
            "Iteration 756, loss = 0.00176411\n",
            "Iteration 757, loss = 0.00177193\n",
            "Iteration 758, loss = 0.00180709\n",
            "Iteration 759, loss = 0.00182203\n",
            "Iteration 760, loss = 0.00182753\n",
            "Iteration 761, loss = 0.00188590\n",
            "Iteration 762, loss = 0.00184803\n",
            "Iteration 763, loss = 0.00187622\n",
            "Iteration 764, loss = 0.00168031\n",
            "Iteration 765, loss = 0.00186249\n",
            "Iteration 766, loss = 0.00168316\n",
            "Iteration 767, loss = 0.00189368\n",
            "Iteration 768, loss = 0.00187434\n",
            "Iteration 769, loss = 0.00173233\n",
            "Iteration 770, loss = 0.00184042\n",
            "Iteration 771, loss = 0.00172636\n",
            "Iteration 772, loss = 0.00169825\n",
            "Iteration 773, loss = 0.00176472\n",
            "Iteration 774, loss = 0.00166699\n",
            "Iteration 775, loss = 0.00169801\n",
            "Iteration 776, loss = 0.00167532\n",
            "Iteration 777, loss = 0.00171011\n",
            "Iteration 778, loss = 0.00168948\n",
            "Iteration 779, loss = 0.00186669\n",
            "Iteration 780, loss = 0.00169919\n",
            "Iteration 781, loss = 0.00165647\n",
            "Iteration 782, loss = 0.00165770\n",
            "Iteration 783, loss = 0.00162642\n",
            "Iteration 784, loss = 0.00167862\n",
            "Iteration 785, loss = 0.00165000\n",
            "Iteration 786, loss = 0.00159590\n",
            "Iteration 787, loss = 0.00169091\n",
            "Iteration 788, loss = 0.00162909\n",
            "Iteration 789, loss = 0.00171745\n",
            "Iteration 790, loss = 0.00154700\n",
            "Iteration 791, loss = 0.00163126\n",
            "Iteration 792, loss = 0.00170439\n",
            "Iteration 793, loss = 0.00166485\n",
            "Iteration 794, loss = 0.00170763\n",
            "Iteration 795, loss = 0.00159064\n",
            "Iteration 796, loss = 0.00161425\n",
            "Iteration 797, loss = 0.00154733\n",
            "Iteration 798, loss = 0.00160245\n",
            "Iteration 799, loss = 0.00166768\n",
            "Iteration 800, loss = 0.00186165\n",
            "Iteration 801, loss = 0.00159225\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"â¸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"â¾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yP6B0zVCHSVd",
        "outputId": "e391e353-32b8-4499-8939-50715cd7d04a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gaImo28XHyiv",
        "outputId": "2722eba5-c725-49ec-dc0a-4cf7172975e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me68652NH1Wv",
        "outputId": "c888729d-c2a9-4a7d-9f91-e560c72fc379"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "bqhf5ghAH9it",
        "outputId": "92014da2-23ff-453b-b889-1fa9990506e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHWgYYK_IAE1",
        "outputId": "786ef884-68e9-45a9-9473-cff203d7e29a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      1.00      0.99        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}